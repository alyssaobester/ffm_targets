---
title: "Updating Functional Flow Predictions"
description: |
  Steps to use revised watershed data to generate functional flow predictions
author:
  - name: Ryan Peek 
    affiliation: Center for Watershed Sciences, UCD
    affiliation_url: https://watershed.ucdavis.edu
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    toc: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, tidy = FALSE, message = FALSE, warning = FALSE)

library(knitr)
library(here)
library(glue)
library(sf)
suppressPackageStartupMessages(library(tidyverse))
library(tmap)
library(tmaptools)
library(mapview)
mapviewOptions(fgb = FALSE)

# data
catch <- read_rds(here("data_input/catchments_final_lshasta.rds"))
catch_clean <- read_rds("data_output/cleaned_full_lshasta_nhd_catchments.rds")
h10 <- read_rds(here("data_input/huc10_little_shasta.rds"))
gages <- read_rds(here("data_input/gages_little_shasta.rds"))
springs <- read_rds(here("data_input/springs_little_shasta.rds"))
lsh_flowlines <- read_rds("data_output/cleaned_full_lshasta_nhd_flownet.rds")
lois <- lsh_flowlines %>%
  filter(ID %in% c(3917946, 3917950, 3917198))

# change to include full delineation RMd or not
FULL_MODE <- TRUE

# https://bookdown.org/yihui/rmarkdown-cookbook/child-document.html

```

# Overview 

This document outlines the process used to generate revised functional flow metric predictions for a given watershed. 
The process outlined below is what is required to generate new predictions for the Little Shasta River case study, but the same steps could be followed in other watersheds too. 
Generally this requires revising the catchment and streamline delineations, re-generating catchment and watershed scale data used in functional flow models, calculating the accumulated values for for each stream segment (`COMID`), and finally running the functional flow predictive model to generate revised flow predictions for the functional flow metrics.

Each step is detailed below, with the associated requirements and potential issues required to consider each step.

```{r makestaticcatchmap, echo=FALSE, eval=FALSE, fig.cap="Revised catchments associated with cleaned streamlines in the Little Shasta.", layout="l-page"}

tmap_mode("plot")
#gm_osm <- read_osm(catch_clean, type = "esri-topo", raster=TRUE)

#tm_shape(gm_osm) + tm_rgb() +
tm_shape(catch_clean) + 
  tm_polygons(col="gray", border.lwd = 0.3, border.alpha = 0.4, alpha=0.2, border.col = "gray30") +
  tm_shape(lois) + 
  tm_sf(col="orange", lwd=8) +
  tm_shape(lsh_flowlines) + tm_lines(col="royalblue2", lwd=2) +
  tm_compass(type = "4star", position = c("left","bottom")) +
  tm_scale_bar(position = c("left","bottom")) +
  tm_layout(frame = FALSE,
            #legend.position = c("left", "top"), 
            title = 'Little Shasta Watershed', 
            #title.position = c('right', 'top'),
            legend.outside = FALSE, 
            attr.outside = FALSE,
            fontfamily = "Roboto Condensed",
            #inner.margins = 0.01, outer.margins = (0.01),
            #legend.position = c(0.6,0.85),
            title.position = c(0.7, 0.95))

# works with roboto  
# tmap::tmap_save(filename = "figures/tmap_lshasta.png", width = 10, height = 8, dpi=300)

# doesn't work with roboto
#tmap::tmap_save(filename = "figs/tmap_lshasta.pdf", width = 10, height = 8, dpi=300)

# crop
# tinytex::tlmgr_install('pdfcrop')
# knitr::plot_crop("figures/tmap_lshasta.png")
```

```{r staticcatchmap, echo=FALSE, eval=FALSE, fig.cap="Revised catchments associated with cleaned streamlines in the Little Shasta.", layout="l-page"}

knitr::include_graphics(here("figures/tmap_lshasta.png"))

```


```{r, child=if(FULL_MODE) 'appendix01_catchment_delineation.Rmd'}
```

# Download Catchment Attributes

With the cleaned flowline network and catchment map, we can proceed with downloading catchment attributes to be used in the flow models. These data were downloaded from ScienceBase, "Select Attributes for NHDPlus Version 2.1 Reach Catchments and Modified Network Routed Upstream Watersheds for the Conterminous United States" (Wieczorek, Jackson and Schwarz, 2018, https://doi.org/10.5066/P9PA63SM).

 - [NHD Flowline Routing](https://www.sciencebase.gov/catalog/item/5b92790be4b0702d0e809fe5)
 - [Select Attributes](https://www.sciencebase.gov/catalog/item/5669a79ee4b08895842a1d47)
 - [Climate and Water Balance Attributes](https://www.sciencebase.gov/catalog/item/566f6c76e4b09cfe53ca77fe)
 - [Krug Average Annual Runoff](https://water.usgs.gov/GIS/metadata/usgswrd/XML/runoff.xml) (and [zipped data file](https://water.usgs.gov/GIS/dsdl/runoff.e00.zip))


## Accumulation for each `COMID` and covariate

Accumulation of the catchment variables or covariates was done using variable appropriate calculations. Primarily this was an area weighted average, calculated as the area of a given COMID catchment, divided by the total accumulated drainage area for that catchment. So the sum of `variable * [(local_catch_area) / (drain_area)]`. Thus for headwater catchments, the area weight should equal **`1`**. 

The original data and crosswalked information is in this [file on github](https://github.com/ryanpeek/ffm_accumulation/blob/main/data_clean/08_accumulated_final_xwalk.csv). There are over 250+ variables in the dataset used for the accumulation. We need to join all these variables to the updated network and catchment areas.

## Calculate Accumulation

First we need to use the cross walk to identify which variables require what type of calculation for the accumulation. Most will use the area weighted average, some require a min or max. 

```{r accumVarSel, eval=FALSE, echo=TRUE}

# crosswalk of variables
xwalk <- read_csv(here("data_input/08_accumulated_final_xwalk.csv"))

vars_awa <- xwalk %>% 
  filter(accum_op_class == "AWA") %>% 
  select(mod_input_final) %>% 
  mutate(dat_output = case_when(
    mod_input_final == "ann_min_precip_basin" ~ "cat_minp6190",
    mod_input_final == "ann_max_precip_basin" ~ "cat_maxp6190",
    mod_input_final == "cat_ppt7100_ann" ~ "pptavg_basin",
    mod_input_final == "et_basin" ~ "cat_et",
    mod_input_final == "wtdepave" ~ "cat_wtdep",
    mod_input_final == "ann_min_precip" ~ "cat_minp6190",
    mod_input_final == "ann_max_precip" ~ "cat_maxp6190",
    mod_input_final == "pet_basin" ~ "cat_pet", 
    mod_input_final == "rh_basin" ~ "cat_rh",
    mod_input_final == "pptavg_basin" ~"cat_ppt7100_ann",
    TRUE ~ mod_input_final), 
    .after = mod_input_final)

# filter to vars
cat_df_awa <- catch_dat %>%
  select(comid:wa_yr, vars_awa$dat_output)

```

Now do the accumulation. Here we pass the list of comids for each comid to a loop/`purrr::map` call to calculate each variable of interest that needs an area weighted average.



<!--
## Test Approach

Try using a known reference reach, pull the data and regenerate the accumulation to see if it actually matches with the metdata and modeled predictions. A small watershed may be suitable (see Hat Creek or upper Sac?). 

 - Hat Creek: (COMID: 7952754, gage T11355500)
 - Upper Sacramento: (COMID: 7964867 (T11341400)
-->
