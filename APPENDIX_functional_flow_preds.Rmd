---
title: "Updating Functional Flow Predictions"
description: |
  Steps to use revised watershed data to generate functional flow predictions
author:
  - name: Ryan Peek 
    affiliation: Center for Watershed Sciences, UCD
    affiliation_url: https://watershed.ucdavis.edu
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    toc: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, tidy = FALSE, message = FALSE, warning = FALSE)

library(knitr)
library(here)
library(glue)
library(sf)
suppressPackageStartupMessages(library(tidyverse))
library(tmap)
library(tmaptools)
library(mapview)
mapviewOptions(fgb = FALSE)

# data
catch <- read_rds(here("data_input/catchments_final_lshasta.rds"))
catch_clean <- read_rds("data_output/cleaned_full_lshasta_nhd_catchments.rds")
h10 <- read_rds(here("data_input/huc10_little_shasta.rds"))
gages <- read_rds(here("data_input/gages_little_shasta.rds"))
springs <- read_rds(here("data_input/springs_little_shasta.rds"))
lsh_flowlines <- read_rds("data_output/cleaned_full_lshasta_nhd_flownet.rds")
lois <- lsh_flowlines %>%
  filter(ID %in% c(3917946, 3917950, 3917198))

# change to include full delineation RMd or not
FULL_MODE <- TRUE

# https://bookdown.org/yihui/rmarkdown-cookbook/child-document.html

```

# Overview

This document outlines the process used to generate revised functional flow metric predictions for a given watershed. The process outlined below is what is required to generate new predictions for the Little Shasta River case study, but the same steps could be followed in other watersheds too. Generally this requires revising the catchment and streamline delineations, re-generating catchment and watershed scale data used in functional flow models, calculating the accumulated values for for each stream segment (`COMID`), scaling or transforming these data, and finally running the functional flow predictive model to generate revised flow predictions for the functional flow metrics.

Each step is detailed below, with the associated requirements and potential issues required to consider each step.

```{r makestaticcatchmap, echo=FALSE, eval=FALSE, fig.cap="Revised catchments associated with cleaned streamlines in the Little Shasta.", layout="l-page"}

tmap_mode("plot")
#gm_osm <- read_osm(catch_clean, type = "esri-topo", raster=TRUE)

#tm_shape(gm_osm) + tm_rgb() +
tm_shape(catch_clean) + 
  tm_polygons(col="gray", border.lwd = 0.3, border.alpha = 0.4, alpha=0.2, border.col = "gray30") +
  tm_shape(lois) + 
  tm_sf(col="orange", lwd=8) +
  tm_shape(lsh_flowlines) + tm_lines(col="royalblue2", lwd=2) +
  tm_compass(type = "4star", position = c("left","bottom")) +
  tm_scale_bar(position = c("left","bottom")) +
  tm_layout(frame = FALSE,
            #legend.position = c("left", "top"), 
            title = 'Little Shasta Watershed', 
            #title.position = c('right', 'top'),
            legend.outside = FALSE, 
            attr.outside = FALSE,
            fontfamily = "Roboto Condensed",
            #inner.margins = 0.01, outer.margins = (0.01),
            #legend.position = c(0.6,0.85),
            title.position = c(0.7, 0.95))

# works with roboto  
# tmap::tmap_save(filename = "figures/tmap_lshasta.png", width = 10, height = 8, dpi=300)

# doesn't work with roboto
#tmap::tmap_save(filename = "figs/tmap_lshasta.pdf", width = 10, height = 8, dpi=300)

# crop
# tinytex::tlmgr_install('pdfcrop')
# knitr::plot_crop("figures/tmap_lshasta.png")
```

```{r staticcatchmap, echo=FALSE, eval=FALSE, fig.cap="Revised catchments associated with cleaned streamlines in the Little Shasta.", layout="l-page"}

knitr::include_graphics(here("figures/tmap_lshasta.png"))

```

```{r, child=if(FULL_MODE) 'appendix01_catchment_delineation.Rmd'}
```

# Download Catchment Attributes

With the cleaned flowline network and catchment map, we can proceed with downloading catchment attributes to be used in the flow models. These data were downloaded from ScienceBase, "Select Attributes for NHDPlus Version 2.1 Reach Catchments and Modified Network Routed Upstream Watersheds for the Conterminous United States" (Wieczorek, Jackson and Schwarz, 2018, <https://doi.org/10.5066/P9PA63SM>).

-   [NHD Flowline Routing](https://www.sciencebase.gov/catalog/item/5b92790be4b0702d0e809fe5)
-   [Select Attributes](https://www.sciencebase.gov/catalog/item/5669a79ee4b08895842a1d47)
-   [Climate and Water Balance Attributes](https://www.sciencebase.gov/catalog/item/566f6c76e4b09cfe53ca77fe)
-   [Krug Average Annual Runoff](https://water.usgs.gov/GIS/metadata/usgswrd/XML/runoff.xml) (and [zipped data file](https://water.usgs.gov/GIS/dsdl/runoff.e00.zip))

## Scaling Data

After subcatchment and basin level attributes have been downloaded, a common step is centering or scaling data to provide a more common set of units and magnitudes so the modeling can be more accurate. In this specific case, the exact scaling used in the original flow models remains unknown, and thus we currently cannot recreate the exact same accumulation dataset used in the original flow models. Scaling is typical with many different types of models, and provides better performance when variance (and data) all fall around a similar variance, and the scales aren't significantly different. 

However, once a known scaling factor is described, accumulation and associated surface water functional flow metrics should be reproducible.

## Accumulation by `COMID` and covariate

Accumulation of the catchment variables was done using variable appropriate calculations (i.e., area weighted average, sum, max, min, etc.). Area weighted average--calculated as the local area of a given COMID catchment, divided by the total accumulated drainage area for that catchment--was the primary calculation for the majority of the metrics (`area_weight = [(local_catch_area) / (drain_area)]`). In headwater catchments, the area weight should equal **`1`**, as the local catchment area and the cumulative drainage area are the same.

The original data and cross-walked information is in this [file on github](https://github.com/ryanpeek/ffm_accumulation/blob/main/data_clean/08_accumulated_final_xwalk.csv). There are over 250+ variables in the dataset used for the accumulation. We need to join all these variables to the updated network and catchment areas.

## Calculate Accumulation by Variable Type

Using the cross walk to identify which variables require what type of calculation for the accumulation, code was used to calculate the accumulation values for each year for each `COMID` for each variable of interest. These calculations used the raw dataset that was downloaded and trimmed from the national NHD dataset described above.

For example, these are variables that used the area weighted average (*`AWA`*): 

```{r accumVarSel, eval=FALSE, echo=TRUE}

# crosswalk of variables
xwalk <- read_csv(here("data_input/08_accumulated_final_xwalk.csv"))

vars_awa <- xwalk %>% 
  filter(accum_op_class == "AWA") %>% 
  select(mod_input_final) %>% 
  mutate(dat_output = case_when(
    mod_input_final == "ann_min_precip_basin" ~ "cat_minp6190",
    mod_input_final == "ann_max_precip_basin" ~ "cat_maxp6190",
    mod_input_final == "cat_ppt7100_ann" ~ "pptavg_basin",
    mod_input_final == "et_basin" ~ "cat_et",
    mod_input_final == "wtdepave" ~ "cat_wtdep",
    mod_input_final == "ann_min_precip" ~ "cat_minp6190",
    mod_input_final == "ann_max_precip" ~ "cat_maxp6190",
    mod_input_final == "pet_basin" ~ "cat_pet", 
    mod_input_final == "rh_basin" ~ "cat_rh",
    mod_input_final == "pptavg_basin" ~"cat_ppt7100_ann",
    TRUE ~ mod_input_final), 
    .after = mod_input_final)

# filter to vars
cat_df_awa <- catch_dat %>%
  select(comid:wa_yr, vars_awa$dat_output)

```

Using the list of `COMIDs` generated previously (for each `COMID` there is a list of all upstream `COMIDs`), the accumulation calculation can be looped and applied to calculate each variable of interest. These data are then collated by `COMID` and year, and then used in the functional flow models.

# Functional Flow Modeling & Future Steps

The functional flow modeling process and framework is described in Grantham et al. (2022) and Yarnell et al. (2021). Using code and the parameters derived from these studies, functional flow models were created for each metric independently. Therefore 24 individual models existed for each of the 24 individual flow metrics. The reference dataset used to derive these models is described in Grantham et al. 2022. Reference models were then used to make flow metric predictions using the Little Shasta River accumulated dataset described above. Data were predicted for each year, and the median value was used to estimate the percentile (e.g., 10, 25, 50, 75, 90) values.

Ultimately, the process identified above is functional and can be used broadly in surface-water driven systems. In the Little Shasta River, the upstream location of interest (LOI-3) was used as it is least impacted by both surfacewater/groundwater interactions, and the functional flow predictions were unaffected by the errors identified in the NHD flow network. 

## Reproducibility

The framework to derive this entire analysis was built on the `{targets}` package in R (version 4.1.3), which permits singular changes at any point in the workflow, but allows the user to rerun only the components which are subsequently affected.

The code and writeups can be found here: https://github.com/ryanpeek/ffm_targets


